# Ardha - AI-Native Project Management Platform

## Project Overview
**Ardha** is the world's first truly unified AI-native project management and development platform that eliminates the artificial boundary between planning and execution.

**Repository:** `/home/veda/ardha-projects/Ardha`  
**GitHub:** https://github.com/ardhaecosystem/Ardha  
**License:** MIT (Open Source)  
**Current Phase:** Pre-Development (Infrastructure Complete)  
**Target MVP:** February 2026 (4 months / 20 weeks)

## Core Problem
Development teams waste 58% of their time coordinating between fragmented tools. Current tools force a false choice:
- **Project Management** (Linear, Jira, Notion) - Planning without execution capability
- **AI Development** (Cursor, Cline, Replit) - Coding without project context

## Revolutionary Solution
Ardha eliminates this dichotomy with **unified workflows** where AI understands BOTH project goals AND implementation details in a single platform.

## Critical Constraints

### Budget Constraint
- **$60/month** for AI operations via OpenRouter
- Token efficiency is paramount
- Model routing: Gemini Flash (35%), Claude Sonnet 4.5 (45%), Claude Opus 4.1 (20%)
- Prompt caching enabled for 78-90% cost reduction

### Memory Constraint
- **8GB total system RAM** across all containers
- PostgreSQL: 2GB limit
- Qdrant: 2.5GB limit (with scalar quantization)
- Redis: 512MB limit (LRU eviction policy)
- Backend: 2GB limit
- Frontend: 1GB limit

### Timeline
- **20 weeks total**, 6 development phases
- Phase 1 (Weeks 1-3): Backend Foundation
- Phase 2 (Weeks 4-6): AI Integration & LangGraph
- Phase 3 (Weeks 7-9): OpenSpec & Git Integration
- Phase 4 (Weeks 10-12): Databases & Background Jobs
- Phase 5 (Weeks 13-16): Frontend Development
- Phase 6 (Weeks 17-20): Integration & Launch Prep

### Scale Target
- 50K+ lines of code, 100+ files
- Backend: ~40 files (routes, services, repositories, models)
- Frontend: ~60 files (pages, components, hooks, utilities)

## Core Innovation

### Unified AI Workflow
- Idea â†’ PRD â†’ Tasks â†’ Implementation (all in one platform)
- OpenSpec-driven development with deterministic workflows
- Project-based memory system that learns from conversations, commits, decisions
- Multi-mode AI: Research, Architect, Implementation, Debug, Documentation

### Cursor-Inspired Project Chat
- Integrated code editor, terminal, and file tree
- Full project context: files, git history, tasks, specs
- Real-time collaboration with AI
- Seamless switching between modes

## Key Features

### AI-Powered
- Multi-model access via OpenRouter (400+ models)
- LangGraph workflow orchestration
- Project-based memory (Redis + Qdrant)
- Prompt caching for cost reduction
- Complexity-based model routing

### Project Management
- Dual chat system (Normal + Project-integrated)
- Task management with Kanban, List, Calendar, Timeline views
- Notion-style databases with dynamic properties
- OpenSpec integration for spec-driven development
- Git and GitHub integration

### Developer Experience
- CodeMirror 6 editor with syntax highlighting
- Integrated terminal (xterm.js)
- Real-time collaboration via WebSocket
- Premium dark/light theme throughout
- Command palette (Cmd+K)

## Success Metrics (MVP - 4 Months)

### Technical
- Page load: <2s (90th percentile)
- API response: <500ms (95th percentile)
- Bundle size: <200KB gzipped
- 99% uptime, <0.1% error rate

### AI Quality
- Task generation accuracy: >80%
- Code implementation success: >70%
- AI cost per project: <$5
- AI response relevance: >90%

### Adoption
- 1,000 GitHub stars
- 100 monthly active users
- 100 projects created
- 50 PRs generated by AI

## Development Philosophy

### Backend-First Strategy
1. Define database schema and API contracts
2. Implement backend with tests
3. Frontend develops against stable API
4. Benefits: Parallel work, stable contracts, faster debugging

### OpenSpec Workflow (Avoid "Vibe Coding")
1. Create specification before complex features (>50 LOC)
2. Lock specs after 2-3 iterations
3. Implement from specs (not vibes)
4. Results: 59% fewer errors, 100K-200K tokens saved per feature

### Test-Driven Development
- Business logic: 90%+ coverage
- API endpoints: 100% coverage (all success/error paths)
- UI components: Critical paths + edge cases

### Token Efficiency
- Hierarchical context loading (5-15K vs 150-300K tokens)
- Prompt caching (78-90% cost reduction)
- Session discipline (stop after 3 failed attempts)
- Memory bank maintenance (prevents context drift)

### Git Safety
- Feature branches only (never commit to main)
- Branch naming: `feature/*`, `bugfix/*`, `hotfix/*`
- Conventional commits format
- PR reviews required before merge

## Technology Stack (Locked Versions)

### Backend
- Python 3.12.3, FastAPI 0.115.4
- SQLAlchemy 2.0.35, Alembic 1.13.3
- PostgreSQL 15, Redis 7.2, Qdrant 1.7.4
- LangChain 0.3.7, LangGraph 0.2.45
- OpenAI SDK (OpenRouter compatible)

### Frontend
- Next.js 15.0.2, React 19 RC
- TypeScript 5.6.3, Tailwind CSS 3.4.14
- CodeMirror 6, xterm.js 5.3.0
- Radix UI, Lucide React, Framer Motion

### Infrastructure
- Docker Compose for deployment
- Caddy for automatic HTTPS
- GitHub Actions for CI/CD

## Current Status (November 5, 2025)

### Completed âœ…
- Monorepo initialized at `/home/veda/ardha-projects/Ardha`
- Git repository with branches: main, dev, feature/initial-setup
- All dependencies locked via Poetry (backend) and pnpm (frontend)
- Shared caches configured (.poetry-cache, .pnpm-store) - 900MB saved
- OpenSpec initialized with comprehensive PRD (123KB project.md)
- GitHub repository created and pushed
- Context7 MCP configured (framework documentation access)
- GitHub MCP configured (repository automation)

### Active Focus ðŸŽ¯
- Initializing Kilo Code memory bank
- Preparing for Phase 1: Backend Foundation (Weeks 1-3)

### Next Immediate Steps
1. Complete memory bank initialization
2. Create backend directory structure
3. Implement authentication system (JWT, OAuth2)
4. Set up database models and migrations
5. Create first OpenSpec change proposal
